{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiagopecurto/tiagopecurto/blob/main/70705-Q6.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Sistemas para Processamento de Big Data\n",
        "## TP1 - Energy Meter Monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The sensor data corresponds to regular readings from 11 residential energy meters. The data covers the month of February 2024.\n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "timestamp | sensor_id | energy\n",
        "----------|-------------|-----------\n",
        "timestamp | string  | float\n",
        "\n",
        "Each energy value (KWh) corresponds to the accumulated value of the meter at the time of measurement. As such,\n",
        "each meter is expected to produce a monotonically increasing series of pairs of timestamp and energy consummed up to that moment.\n",
        "\n",
        "The meters do not start at zero or at the same value.\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "The following questions should be answered for the month of February and only for this month.\n",
        "\n",
        "### For the group of sensors:\n",
        "\n",
        "1. Compute the total energy consumed.\n",
        "\n",
        "2. Compute the running total energy consumed so far for each day, inclusive.\n",
        "\n",
        "Note: You can approximate the result but using the last reading of each day from each sensor.\n",
        "\n",
        "### For each sensor, separately:\n",
        "\n",
        "3. Compute the total energy consumed and the average energy consumption per day.\n",
        "\n",
        "4. Compute the day of the month with minimum and maximum energy consumption.\n",
        "\n",
        "Note: You can approximate the result but using the last reading of each day from each sensor.\n",
        "\n",
        "### For each sensor, separately, with estimations:\n",
        "\n",
        "**Assumptions:**\n",
        "\n",
        "+ Readings may be missing for extended periods due to communication problems with the sensors.\n",
        "\n",
        "+ Readings are collected do not fall precisely \"on the hour\". The are collected and recorded any time.\n",
        "\n",
        "+ For more precise results, estimate the value of the meter at precise timestamp, using linear interpolation from nearest readings.\n",
        "\n",
        "5. Compute the **estimated** value of each sensor meter for every hour and day of the month (in ascending order).\n",
        "\n",
        "6. Compute the **estimated** running total of the energy consumed so far. The value should be updated every hour."
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "Solve each question using Structured Spark, either Dataframes or SQL or both."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Grading Criteria\n",
        "\n",
        "+ Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "+ November 10, 23h59\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619bf040-719b-4a0e-ed60-b925f43925e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "\n",
        "!wget -q -O energy-readings.csv https://raw.githubusercontent.com/smduarte/spbd-2425/refs/heads/main/docs/labs/projs/energy-readings.csv\n",
        "!head -2 energy-readings.csv"
      ],
      "metadata": {
        "id": "jB8R09NjYJUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('energy').getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "    readings = spark.read.csv('energy-readings.csv', sep =';', header=True, inferSchema=True)\n",
        "\n",
        "    #0.Filtra os dados para manter apenas o mês de fevereiro de 2024\n",
        "    readings_february = readings.filter((month(\"date\") == 2) & (year(\"date\") == 2024))\n",
        "\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "id": "zo41adHJYGs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 6** - For each sensor, separately, with estimations compute the estimated running total of the energy consumed so far. The value should be updated every hour."
      ],
      "metadata": {
        "id": "AmUjHREKSNF5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from datetime import datetime\n",
        "\n",
        "# 5.0 Função de Interpolação Linear - Adaptada de StackOverflow \"Pyspark : Interpolation of missing values in pyspark dataframe observed\"\n",
        "    #Link: https://stackoverflow.com/questions/53077639/pyspark-interpolation-of-missing-values-in-pyspark-dataframe-observed\n",
        "def fill_linear_interpolation(df, id_cols, order_col, value_col):\n",
        "    w = Window.partitionBy(id_cols).orderBy(order_col)\n",
        "    new_df = df.withColumn('rn', F.row_number().over(w))\n",
        "    new_df = new_df.withColumn('rn_not_null', F.when(F.col(value_col).isNotNull(), F.col('rn')))\n",
        "\n",
        "    w_start = Window.partitionBy(id_cols).orderBy(order_col).rowsBetween(Window.unboundedPreceding, -1)\n",
        "    new_df = new_df.withColumn('start_val', F.last(value_col, True).over(w_start))\n",
        "    new_df = new_df.withColumn('start_rn', F.last('rn_not_null', True).over(w_start))\n",
        "\n",
        "    w_end = Window.partitionBy(id_cols).orderBy(order_col).rowsBetween(0, Window.unboundedFollowing)\n",
        "    new_df = new_df.withColumn('end_val', F.first(value_col, True).over(w_end))\n",
        "    new_df = new_df.withColumn('end_rn', F.first('rn_not_null', True).over(w_end))\n",
        "\n",
        "    if not isinstance(id_cols, list):\n",
        "        id_cols = [id_cols]\n",
        "\n",
        "    new_df = new_df.withColumn('diff_rn', F.col('end_rn') - F.col('start_rn'))\n",
        "    new_df = new_df.withColumn('curr_rn', F.col('diff_rn') - (F.col('end_rn') - F.col('rn')))\n",
        "\n",
        "    lin_interp_func = (F.col('start_val') + (F.col('end_val') - F.col('start_val')) / F.col('diff_rn') * F.col('curr_rn'))\n",
        "    new_df = new_df.withColumn(value_col, F.when(F.col(value_col).isNull(), lin_interp_func).otherwise(F.col(value_col)))\n",
        "\n",
        "    return new_df.drop('rn', 'rn_not_null', 'start_val', 'end_val', 'start_rn', 'end_rn', 'diff_rn', 'curr_rn')\n",
        "\n",
        "# 6.1. Filtrar apenas os dados do mês de fevereiro de 2024\n",
        "readings_february = readings.filter((F.month(\"date\") == 2) & (F.year(\"date\") == 2024)).orderBy(\"sensor\", \"date\")\n",
        "\n",
        "# 6.2. Reduzir a precisão de `date` para a hora (YYYY-MM-DD HH:00:00), para agrupar por sensor e hora\n",
        "readings_february = readings_february.withColumn(\"hour\", F.date_trunc(\"hour\", \"date\")).orderBy(\"sensor\", \"hour\")\n",
        "\n",
        "# 6.3. Para cada sensor e cada hora, obter o valor máximo de energia, descartando múltiplas contagens na mesma hora\n",
        "max_hourly_readings = readings_february.groupBy(\"sensor\", \"hour\").agg(F.max(\"energy\").alias(\"energy\"))\n",
        "\n",
        "# 6.4. Criar uma sequência de timestamps para cada hora no mês de fevereiro de 2024\n",
        "start_timestamp = datetime(2024, 2, 1, 0, 0, 0)\n",
        "end_timestamp = datetime(2024, 2, 29, 23, 0, 0)\n",
        "\n",
        "# 6.5 Gera uma sequência de timestamps a cada hora\n",
        "timestamp_df = spark.createDataFrame([(start_timestamp, end_timestamp)], [\"start_timestamp\", \"end_timestamp\"]) \\\n",
        "    .select(F.explode(F.sequence(\"start_timestamp\", \"end_timestamp\", F.expr(\"INTERVAL 1 HOUR\"))).alias(\"timestamp\"))\n",
        "\n",
        "# 6.6. Fazer um cross join entre os sensores e os timestamps de fevereiro para garantir uma série completa\n",
        "sensors_df = max_hourly_readings.select(\"sensor\").distinct()\n",
        "all_timestamps_for_sensors = sensors_df.crossJoin(timestamp_df)\n",
        "\n",
        "# 6.7. Realizar o left join para preencher com dados reais onde disponíveis e `null` onde faltam contagens\n",
        "complete_hourly_data = all_timestamps_for_sensors.join(\n",
        "    max_hourly_readings,\n",
        "    on=(all_timestamps_for_sensors[\"sensor\"] == max_hourly_readings[\"sensor\"]) &\n",
        "       (all_timestamps_for_sensors[\"timestamp\"] == max_hourly_readings[\"hour\"]),\n",
        "    how=\"left\"\n",
        ")\n",
        "\n",
        "# 6.8. Selecionar as colunas finais e renomear para remover ambiguidades\n",
        "complete_hourly_data = complete_hourly_data.select(\n",
        "    all_timestamps_for_sensors[\"sensor\"],\n",
        "    all_timestamps_for_sensors[\"timestamp\"],\n",
        "    max_hourly_readings[\"energy\"]\n",
        ").orderBy(\"sensor\", \"timestamp\")\n",
        "\n",
        "\n",
        "interpolated_hourly_data = fill_linear_interpolation(complete_hourly_data, id_cols=\"sensor\", order_col=\"timestamp\", value_col=\"energy\")\n",
        "\n",
        "readings_february_interpolated = interpolated_hourly_data.filter((F.month(\"timestamp\") == 2) & (F.year(\"timestamp\") == 2024))\n",
        "\n",
        "consumption_february = readings_february_interpolated.groupBy(\"sensor\").agg(\n",
        "    F.first(\"energy\").alias(\"start_energy\"),\n",
        "    F.last(\"energy\").alias(\"end_energy\")\n",
        ")\n",
        "consumption_february = consumption_february.withColumn(\"total_consumption\", round(F.col(\"end_energy\") - F.col(\"start_energy\"),3))\n",
        "\n",
        "result = consumption_february.select(\"sensor\", \"total_consumption\").orderBy(\"sensor\")\n",
        "\n",
        "result.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JDKUpG5PkunR",
        "outputId": "79ad3c22-9cfb-4f72-845f-ed75b8482f89"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+------+-----------------+\n",
            "|sensor|total_consumption|\n",
            "+------+-----------------+\n",
            "|     A|           166.38|\n",
            "|     B|           129.71|\n",
            "|     C|           257.22|\n",
            "|     D|            485.9|\n",
            "|     E|           447.76|\n",
            "|     F|           159.71|\n",
            "|     G|           168.37|\n",
            "|     H|            478.1|\n",
            "|     I|           351.41|\n",
            "|     J|           230.05|\n",
            "|     K|            225.5|\n",
            "+------+-----------------+\n",
            "\n"
          ]
        }
      ]
    }
  ]
}