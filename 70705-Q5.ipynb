{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tiagopecurto/tiagopecurto/blob/main/70705-Q5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KgFZlT_g8O1Y"
      },
      "source": [
        "# Sistemas para Processamento de Big Data\n",
        "## TP1 - Energy Meter Monitoring\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "The sensor data corresponds to regular readings from 11 residential energy meters. The data covers the month of February 2024.\n",
        "\n",
        "Each data sample has the following schema:\n",
        "\n",
        "timestamp | sensor_id | energy\n",
        "----------|-------------|-----------\n",
        "timestamp | string  | float\n",
        "\n",
        "Each energy value (KWh) corresponds to the accumulated value of the meter at the time of measurement. As such,\n",
        "each meter is expected to produce a monotonically increasing series of pairs of timestamp and energy consummed up to that moment.\n",
        "\n",
        "The meters do not start at zero or at the same value.\n"
      ],
      "metadata": {
        "id": "IRDJq9dL0GWA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Questions\n",
        "\n",
        "The following questions should be answered for the month of February and only for this month.\n",
        "\n",
        "### For the group of sensors:\n",
        "\n",
        "1. Compute the total energy consumed.\n",
        "\n",
        "2. Compute the running total energy consumed so far for each day, inclusive.\n",
        "\n",
        "Note: You can approximate the result but using the last reading of each day from each sensor.\n",
        "\n",
        "### For each sensor, separately:\n",
        "\n",
        "3. Compute the total energy consumed and the average energy consumption per day.\n",
        "\n",
        "4. Compute the day of the month with minimum and maximum energy consumption.\n",
        "\n",
        "Note: You can approximate the result but using the last reading of each day from each sensor.\n",
        "\n",
        "### For each sensor, separately, with estimations:\n",
        "\n",
        "**Assumptions:**\n",
        "\n",
        "+ Readings may be missing for extended periods due to communication problems with the sensors.\n",
        "\n",
        "+ Readings are collected do not fall precisely \"on the hour\". The are collected and recorded any time.\n",
        "\n",
        "+ For more precise results, estimate the value of the meter at precise timestamp, using linear interpolation from nearest readings.\n",
        "\n",
        "5. Compute the **estimated** value of each sensor meter for every hour and day of the month (in ascending order).\n",
        "\n",
        "6. Compute the **estimated** running total of the energy consumed so far. The value should be updated every hour."
      ],
      "metadata": {
        "id": "HC6tMDOU7Fdb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Requeriments\n",
        "\n",
        "Solve each question using Structured Spark, either Dataframes or SQL or both."
      ],
      "metadata": {
        "id": "kdTj-7SD-67o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Other Grading Criteria\n",
        "\n",
        "+ Grading will also take into account the general clarity of the programming and of the presentation report (notebook).\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qN2ogthr_EIH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deadline\n",
        "+ November 10, 23h59\n",
        "\n",
        "For each day late, ***0.5 / day penalty***. Penalty accumulates until the grade of the assignment reaches 8.0."
      ],
      "metadata": {
        "id": "8M6lYfT_BpAO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Colab Setup\n"
      ],
      "metadata": {
        "id": "81dR9BTgBg1s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Install PySpark\n",
        "!pip install pyspark findspark --quiet"
      ],
      "metadata": {
        "id": "L2O_3I3x1dbx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "619bf040-719b-4a0e-ed60-b925f43925e4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m317.3/317.3 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Download the dataset\n",
        "\n",
        "!wget -q -O energy-readings.csv https://raw.githubusercontent.com/smduarte/spbd-2425/refs/heads/main/docs/labs/projs/energy-readings.csv\n",
        "!head -2 energy-readings.csv"
      ],
      "metadata": {
        "id": "jB8R09NjYJUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import *\n",
        "from pyspark.sql.types import *\n",
        "from pyspark.sql.functions import *\n",
        "\n",
        "spark = SparkSession.builder.master('local[*]') \\\n",
        "\t\t\t\t\t\t.appName('energy').getOrCreate()\n",
        "\n",
        "sc = spark.sparkContext\n",
        "\n",
        "try :\n",
        "    readings = spark.read.csv('energy-readings.csv', sep =';', header=True, inferSchema=True)\n",
        "\n",
        "    #0.Filtra os dados para manter apenas o mês de fevereiro de 2024\n",
        "    readings_february = readings.filter((month(\"date\") == 2) & (year(\"date\") == 2024))\n",
        "\n",
        "except Exception as err:\n",
        "    print(err)"
      ],
      "metadata": {
        "id": "zo41adHJYGs6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Question 5** - Compute the estimated value of each sensor meter for day of the month (in ascending order)."
      ],
      "metadata": {
        "id": "A5pre2dzMW6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql.window import Window\n",
        "from datetime import datetime, timedelta\n",
        "\n",
        "# 5.0 Função de Interpolação Linear - Adaptada de StackOverflow \"Pyspark : Interpolation of missing values in pyspark dataframe observed\"\n",
        "    #Link: https://stackoverflow.com/questions/53077639/pyspark-interpolation-of-missing-values-in-pyspark-dataframe-observed\n",
        "def fill_linear_interpolation(df, id_cols, order_col, value_col):\n",
        "    w = Window.partitionBy(id_cols).orderBy(order_col)\n",
        "    new_df = df.withColumn('rn', F.row_number().over(w))\n",
        "    new_df = new_df.withColumn('rn_not_null', F.when(F.col(value_col).isNotNull(), F.col('rn')))\n",
        "\n",
        "    w_start = Window.partitionBy(id_cols).orderBy(order_col).rowsBetween(Window.unboundedPreceding, -1)\n",
        "    new_df = new_df.withColumn('start_val', F.last(value_col, True).over(w_start))\n",
        "    new_df = new_df.withColumn('start_rn', F.last('rn_not_null', True).over(w_start))\n",
        "\n",
        "    w_end = Window.partitionBy(id_cols).orderBy(order_col).rowsBetween(0, Window.unboundedFollowing)\n",
        "    new_df = new_df.withColumn('end_val', F.first(value_col, True).over(w_end))\n",
        "    new_df = new_df.withColumn('end_rn', F.first('rn_not_null', True).over(w_end))\n",
        "\n",
        "    if not isinstance(id_cols, list):\n",
        "        id_cols = [id_cols]\n",
        "\n",
        "    new_df = new_df.withColumn('diff_rn', F.col('end_rn') - F.col('start_rn'))\n",
        "    new_df = new_df.withColumn('curr_rn', F.col('diff_rn') - (F.col('end_rn') - F.col('rn')))\n",
        "\n",
        "    lin_interp_func = (F.col('start_val') + (F.col('end_val') - F.col('start_val')) / F.col('diff_rn') * F.col('curr_rn'))\n",
        "    new_df = new_df.withColumn(value_col, F.when(F.col(value_col).isNull(), lin_interp_func).otherwise(F.col(value_col)))\n",
        "\n",
        "    return new_df.drop('rn', 'rn_not_null', 'start_val', 'end_val', 'start_rn', 'end_rn', 'diff_rn', 'curr_rn')\n",
        "\n",
        "# 5.1. Cria uma série de datas para todo o mês de fevereiro de 2024\n",
        "date_range_df = spark.createDataFrame([(datetime(2024, 2, 1), datetime(2024, 2, 29))], [\"start_date\", \"end_date\"])\n",
        "date_df = date_range_df.select(explode(sequence(\"start_date\", \"end_date\")).alias(\"day\"))\n",
        "date_df = date_df.withColumn(\"day\", to_date(\"day\"))\n",
        "\n",
        "# 5.2. Obter a leitura máxima diária para cada sensor\n",
        "readings_february = readings_february.withColumn(\"day\", to_date(\"date\"))\n",
        "max_daily_readings = readings_february \\\n",
        "    .groupBy(\"sensor\", \"day\") \\\n",
        "    .agg(max(\"energy\").alias(\"max_energy\")) \\\n",
        "    .orderBy(\"sensor\", \"day\")\n",
        "\n",
        "# 5.3. Realizar um join para garantir que todos os dias de fevereiro estão presentes para cada sensor\n",
        "sensors_df = max_daily_readings.select(\"sensor\").distinct()\n",
        "all_dates_for_sensors = sensors_df.crossJoin(date_df)\n",
        "\n",
        "# 5.4 Left join para completar com dados reais onde disponíveis e null onde não há contagens\n",
        "complete_daily_data = all_dates_for_sensors.join(\n",
        "    max_daily_readings,\n",
        "    on=[\"day\", \"sensor\"],\n",
        "    how=\"left\"\n",
        ").orderBy(\"day\", \"sensor\")\n",
        "\n",
        "# 5.5. Aplicar a função de interpolação linear\n",
        "interpolated_daily_data = fill_linear_interpolation(complete_daily_data, id_cols=\"sensor\", order_col=\"day\", value_col=\"max_energy\")\n",
        "\n",
        "# 5.6. Aplicar a função de interpolação linear\n",
        "interpolated_daily_data = interpolated_daily_data.withColumn(\"max_energy\", round(\"max_energy\",3))\n",
        "\n",
        "# 5.6. Exibir o resultado final com os valores interpolados\n",
        "interpolated_daily_data.show()"
      ],
      "metadata": {
        "id": "GavR91du468j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28f25ecc-141d-4b3b-a402-013decda5a1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+----------+------+----------+\n",
            "|       day|sensor|max_energy|\n",
            "+----------+------+----------+\n",
            "|2024-02-01|     A|     658.6|\n",
            "|2024-02-02|     A|     663.4|\n",
            "|2024-02-03|     A|   670.414|\n",
            "|2024-02-04|     A|   677.429|\n",
            "|2024-02-05|     A|   684.443|\n",
            "|2024-02-06|     A|   691.457|\n",
            "|2024-02-07|     A|   698.471|\n",
            "|2024-02-08|     A|   705.486|\n",
            "|2024-02-09|     A|     712.5|\n",
            "|2024-02-10|     A|     719.9|\n",
            "|2024-02-11|     A|     726.3|\n",
            "|2024-02-12|     A|     733.0|\n",
            "|2024-02-13|     A|     739.8|\n",
            "|2024-02-14|     A|     744.8|\n",
            "|2024-02-15|     A|     749.1|\n",
            "|2024-02-16|     A|     751.0|\n",
            "|2024-02-17|     A|    757.15|\n",
            "|2024-02-18|     A|     763.3|\n",
            "|2024-02-19|     A|     769.0|\n",
            "|2024-02-20|     A|     773.3|\n",
            "+----------+------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    }
  ]
}